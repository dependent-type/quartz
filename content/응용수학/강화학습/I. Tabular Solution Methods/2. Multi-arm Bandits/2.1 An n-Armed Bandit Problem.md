![[Pasted image 20230803094745.png]]

**n-Armed Bandit Problem**, 또는 **Multi-Armed Bandit Problem**은 서로 수익률이 다른 여러 개의 슬롯머신이 있을 때 수익을 최대화하는 문제이다. 과거 카지노에서 어떤 슬롯머신에 게임을 해야 최대한 많은 수익을 얻을 수 있을지에 대한 고민에서 출발했다고 한다.

이 문제에서 우리가 취하는 특정 행동은 그에 대응되는 보상의 기댓값이 존재한다. 이것을 각 행동(action)의 가치(value)라고 하자. 각 행동의 가치를 알고 있다면 문제를 굉장히 자명하게 가장 가치가 높은 행동만을 함으로서 해결할 수 있을 것이다. 따라서 우리는 확실한 value를 알지 못하는 경우에 대해서 생각하자.

만약 우리가 과정 전반에 있어 action에 대한 value들을 계속 추정한다면, 어느 시점을 잡던 그 시점에서의 value가 최대인 action이 있을 것이다. 이때 매 순간마다 그 순간에서의 추정 value값이 높은 action을 **greedy action**이라고 한다. greedy action을 선택하는 것을 **exploiting**한다고 하고, non-greedy action을 선택하는 것을 **exploring**한다고 한다. Exploitation은 현재 알고 있는 정보를 토대로 expected reward를 최대화시킬 수 있는 방법이지만, 더 길게 본다면 과정이 끝났을 때는 exploration이 더 큰 보상을 얻을 수도 있다. 예를 들어 현재의 greedy action은 그 value가 어느정도 특정적이게 알려졌지만 나머지 action들의 경우 굉장히 불확실할 때, 무작정 greedy action을 선택하는 것보다 다른 action들을 더 exploring해보는 것이 장기적으로는 더 이득일 수 있다. 즉, exploring은 long run에서 장점을 가진다. 일반적으로는 exploring과 exploiting을 같이 사용한다. ^efbc75

주어진 상황(현재 얻은 데이터, 불확실성 등)에 따른 이상적인 전략을 짜는 방법은 이미 잘 연구되어 있다. 그러나 이러한 방법의 대부분은 수학적인 계산이 가능하도록 하기 위해 강력한 가정을 하는 경우가 많아 그 가정이 적용되지 않는 실제 케이스에 대해서는 큰 의미가 없다. 


 